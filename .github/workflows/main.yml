name: Convert Model to MLModel

on:
  workflow_dispatch:  # Allows manual triggering
  push:               # Runs when changes are pushed to main
    branches:
      - main
    paths:
      - 'model.tar.gz'  # Only runs when model.tar.gz changes

jobs:
  convert-to-mlmodel:
    runs-on: macos-latest  # macOS runner needed for coremltools

    steps:
      # Checkout the repository
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'  # Use a stable, recent version

      # Cache pip dependencies
      - name: Cache Dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # Install dependencies
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install tensorflow==2.17.0 coremltools==7.2 numpy

      # Extract and convert the model
      - name: Convert to MLModel
        run: |
          # Create directory for extraction
          mkdir -p ./extracted_model

          # Extract the tar.gz file
          tar -xzf ./model.tar.gz -C ./extracted_model/ || { echo "Failed to extract model.tar.gz"; exit 1; }

          # Python script to convert
          python - <<'EOF'
          import tensorflow as tf
          import coremltools as ct
          import os
          import sys
          import numpy as np

          # Find the model directory in extracted files
          model_dir = './extracted_model/'
          model_path = None
          for item in os.listdir(model_dir):
              full_path = os.path.join(model_dir, item)
              if os.path.isdir(full_path):
                  model_path = full_path
                  break
          if not model_path:
              print("Error: No model directory found in extracted files")
              sys.exit(1)

          # The model directory should be 'backdoor-ai-tensorflow1-bdg-v1', but it's still a MobileBERT model
          print(f"Found model directory: {model_path}")

          # Load TensorFlow SavedModel
          try:
              tf_model = tf.saved_model.load(model_path)
          except Exception as e:
              print(f"Error loading TensorFlow SavedModel: {e}")
              sys.exit(1)

          # Define a concrete function for conversion (Core ML requires a concrete function)
          # MobileBERT expects input_ids, attention_mask, and token_type_ids
          try:
              concrete_func = tf_model.signatures["serving_default"]
              if not concrete_func:
                  print("Error: No serving_default signature found in SavedModel")
                  sys.exit(1)
              print("Model signatures:", tf_model.signatures.keys())
              print("Input signature for serving_default:", concrete_func.structured_input_signature)
          except Exception as e:
              print(f"Error retrieving concrete function: {e}")
              sys.exit(1)

          # Define input shapes for MobileBERT (batch size 1, sequence length 128)
          input_shape = (1, 128)  # Batch size 1, sequence length 128
          input_ids = tf.TensorSpec(input_shape, tf.int32, name="input_ids")
          attention_mask = tf.TensorSpec(input_shape, tf.int32, name="attention_mask")
          token_type_ids = tf.TensorSpec(input_shape, tf.int32, name="token_type_ids")

          # Convert to Core ML
          try:
              mlmodel = ct.convert(
                  concrete_func,
                  inputs=[
                      ct.TensorType(name="input_ids", shape=input_shape, dtype=np.int32),
                      ct.TensorType(name="attention_mask", shape=input_shape, dtype=np.int32),
                      ct.TensorType(name="token_type_ids", shape=input_shape, dtype=np.int32),
                  ],
                  source='tensorflow',
                  convert_to='mlmodel'
              )
          except Exception as e:
              print(f"Error converting to Core ML: {e}")
              sys.exit(1)

          # Save the converted model
          output_path = './converted_model.mlmodel'
          mlmodel.save(output_path)
          print(f"Model successfully converted and saved to {output_path}")
          EOF

      # Commit and push the converted model
      - name: Commit and Push to GitHub
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add ./converted_model.mlmodel
          git commit -m "Add converted MLModel [$(date +'%Y-%m-%d %H:%M:%S UTC')]" || echo "No changes to commit"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}